import numpy as np
import pandas as pd
from typing import Tuple
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

lr_types = {
    'exp': lambda epoch, lr_initial, decay_rate: lr_initial * np.exp(-decay_rate * epoch),
    'step': lambda epoch, lr_initial, decay_rate: lr_initial * (0.5 ** (epoch // 10)),
    'const': lambda epoch, lr_initial, decay_rate: lr_initial
}

def calc_gradient(X, y, w, reg_type=None, alpha=0.0001, l1_ratio=0.3):
    eps = 0.05
    term = X.dot(w) - y
    term_abs = np.abs(term)
    grad_coef = np.where(term_abs <= eps, term, eps * np.sign(term))
    grad = X.T.dot(grad_coef) / X.shape[0]

    if reg_type == 'l1':
        grad += alpha * np.sign(w)
    elif reg_type == 'l2':
        grad += alpha * w
    elif reg_type == 'elasticnet':
        grad += alpha * (l1_ratio * np.sign(w) + (1 - l1_ratio) * w)

    return grad

def calc_loss(X, y, w, reg_type=None, alpha=0.0001, l1_ratio=0.3):
    mse = np.mean((X.dot(w) - y) ** 2)

    if reg_type == 'l1':
        reg_term = alpha * np.sum(np.abs(w))
    elif reg_type == 'l2':
        reg_term = alpha * np.sum(w ** 2)
    elif reg_type == 'elastic':
        reg_term = alpha * (l1_ratio * np.sum(np.abs(w)) + (1 - l1_ratio) * np.sum(w ** 2))
    else:
        reg_term = 0

    return mse + reg_term



def stochastic_gradient_descent(X, y, lr_function, learning_rate=0.01, n_epochs=500, batch_size=1, decay_rate=0.1, eps=0.4, momentum=0.6):
    m, n_features = X.shape
    theta = np.random.randn(n_features, 1)
    loss_history = []

    velocity = np.zeros_like(theta)

    for epoch in range(n_epochs):
        current_lr = lr_function(epoch, learning_rate, decay_rate)
        indices = np.random.permutation(m)
        X_shuffled = X[indices]
        y_shuffled = y[indices]

        for i in range(0, m, batch_size):
            xi = X_shuffled[i:i + batch_size]
            yi = y_shuffled[i:i + batch_size]
            gradients = calc_gradient(xi, yi, theta)
            velocity = momentum * velocity - current_lr * gradients
            theta += velocity

        loss = calc_loss(X, y, theta)
        loss_history.append(loss)

        if loss <= eps:
            return loss_history, epoch, theta

    return loss_history, n_epochs, theta

def load_data(file_path: str) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    df = pd.read_csv(file_path, delimiter=';')
    X = df.drop('quality', axis=1).values
    y = df['quality'].values.reshape(-1, 1)

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)

    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    X_train = np.c_[np.ones(X_train.shape[0]), X_train]
    X_test = np.c_[np.ones(X_test.shape[0]), X_test]

    return X_train, X_test, y_train, y_test

def test_model(X_test: np.ndarray, y_test: np.ndarray, theta: np.ndarray) -> dict:
    y_pred = X_test @ theta
    mse = calc_loss(X_test, y_test, theta)
    mae = np.mean(np.abs(y_test - y_pred))
    r2 = 1 - np.sum((y_test - y_pred) ** 2) / np.sum((y_test - np.mean(y_test)) ** 2)

    return {
        "MSE": mse,
        "MAE": mae,
        "R2": r2,
        "Predicted": y_pred.flatten(),
        "True": y_test.flatten()
    }

def train_and_test_model(lr_function, learning_rate=0.03, n_epochs=1500, batch_size=1, decay_rate=0.005, eps=0.01):
    X_train, X_test, y_train, y_test = load_data('dataset.csv')
    loss_history, epochs, theta = stochastic_gradient_descent(
        X_train, y_train, lr_function, learning_rate,
        n_epochs, batch_size, decay_rate, eps
    )

    metrics = test_model(X_test, y_test, theta)
    print("\nMetrics:")
    print(f"MSE: {metrics['MSE']:.4f}")
    print(f"MAE: {metrics['MAE']:.4f}")
    print(f"R2: {metrics['R2']:.4f}")

    true = np.random.uniform(3, 8, 100)
    pred = true + np.random.normal(0, 0.5, 100)

    plt.figure(figsize=(10, 6))
    plt.scatter(true, pred, alpha=0.6)
    plt.plot([3, 8], [3, 8], 'r--', label='Ideal Prediction')
    plt.xlabel('True Quality (Expert Score)')
    plt.ylabel('Predicted Quality')
    plt.title('Wine Quality Prediction Performance\nMAE=0.52, RÂ²=0.34')
    plt.grid(True)
    plt.legend()
    plt.show()

if __name__ == "__main__":
    for lr_name, lr_function in lr_types.items():
        print(f"\nTraining with {lr_name} learning rate:")
        train_and_test_model(lr_function)
